{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181ebd2c-a608-4f99-bf23-7f1b486104eb",
   "metadata": {},
   "source": [
    "# Key Problem IV: Binarizing Classes\n",
    "# Empirically Setting the Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d83e71-82f3-4754-8801-b202d73d9717",
   "metadata": {},
   "source": [
    "Here, we demonstrate how to empirically select the classifier threshold for binarizing classes for group comparisons. For example, let's assume that we'd like to investigate whether tweets containing argumentation strategy opinion are also less hateful than tweets not containing opinion (see also notebook on sampling groups for comparison). We will need to define a classifier threshold to distinguish tweets with \"opinion\" from tweets with \"no opinion\". In the following, we demonstrate how to tune this threshold against classifier F1 scores to make both classes maximally distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8480f57a-dff1-4ec6-9e0b-1e379c1635b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.special import softmax\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e93c8c1-adc5-47a9-ae3c-7c3958d0bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d623932-7aa9-443f-a0d2-3c639afd114d",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a444d8-ad41-4c58-9b1a-72f00af3cf65",
   "metadata": {},
   "source": [
    "### Find the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb61d7f-98eb-4cd7-813e-dfa1442763cc",
   "metadata": {},
   "source": [
    "First, we need to find the best performing model for a certain classification task overall (e.g., classifying argumentation strategy). Remember: We trained models on five different data splits for the same task to select the best out of five (see also notebook on training on confident examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b657bb3-e07b-45e3-8c30-7f3c15ef0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(src, model, class_dict):\n",
    "    \"\"\"\n",
    "    Finds the split that performs best in terms of macro avg F1 score over\n",
    "    all classes (for argumentation startegy) and returns the split number.\n",
    "    Assume we have already generated files with predictions of each of the\n",
    "    models (splits) for our test set (here in directory \"data/preds_strategy_test\").\n",
    "    \"\"\"\n",
    "    pred_files = os.listdir(Path(src, \"preds_strategy_test\"))\n",
    "    pred_files = [f for f in pred_files if model in f]\n",
    "    pred_files.sort()\n",
    "    pred_cols = [f\"raw_pred_{i}\" for i in range(len(class_dict))]\n",
    "    \n",
    "    performances = pd.DataFrame()\n",
    "    for split in range(5):\n",
    "        # load raw predictions of classifier\n",
    "        preds = pd.read_csv(\n",
    "            Path(src, \"preds_strategy_test\", pred_files[split])\n",
    "        )\n",
    "        # apply softmax to turn raw predictions into probabilities\n",
    "        preds[pred_cols] = softmax(preds[pred_cols], axis=1)\n",
    "        preds = preds.rename(columns={\"label\": \"true_label\"})\n",
    "        report = classification_report(\n",
    "            preds[\"true_label\"], \n",
    "            preds[\"pred_label\"], \n",
    "            output_dict=True,\n",
    "            zero_division = 0\n",
    "        )\n",
    "        \n",
    "        row = pd.DataFrame({\n",
    "            \"split\": [split], \n",
    "            \"f1-score\": [report[\"macro avg\"][\"f1-score\"]]\n",
    "        })\n",
    "        \n",
    "        performances = pd.concat([performances, row]).reset_index(drop=True)\n",
    "    \n",
    "    best_split = int(performances.loc[performances.idxmax()[\"f1-score\"]][\"split\"])\n",
    "    \n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868477f-274d-41c9-b628-37969dbdcd2e",
   "metadata": {},
   "source": [
    "### Try Thresholds & Get Optimal Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd64df9-a527-4ab8-9040-f32e53b595ba",
   "metadata": {},
   "source": [
    "After finding the best performing model for argumentation strategy overall, we scan through different thresholds to distinguish for example \"opinion\" and \"not opinion\", calculating the F1 score with respect to the human annotated test set for each of those thresholds. We would like to select the threshold for which the F1 score is the highest to make groups \"opinion\" and \"not opinion\" maximally distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e06155-3629-4396-aaa7-a6bbbb959320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_thresholds(src, model, class_dict, split):\n",
    "    \"\"\"\n",
    "    Scans different thresholds for prediction probabilities to binarize \n",
    "    predictions. Returns the classifier performances for each threshold.\n",
    "    \"\"\"\n",
    "    # load raw predictions of classifier\n",
    "    pred_files = os.listdir(Path(src, \"preds_strategy_test\"))\n",
    "    pred_files = [f for f in pred_files if model in f]\n",
    "    pred_files.sort()\n",
    "    pred_cols = [f\"raw_pred_{i}\" for i in range(len(class_dict))]\n",
    "    preds = pd.read_csv(\n",
    "        Path(src, \"preds_strategy_test\", pred_files[split])\n",
    "    )\n",
    "    # apply softmax to turn raw predictions into probabilities\n",
    "    preds[pred_cols] = softmax(preds[pred_cols], axis=1)\n",
    "    preds = preds.rename(columns={\"label\": \"true_label\"})\n",
    "\n",
    "    thresholds = np.arange(0, 1, 0.01)\n",
    "    performances = pd.DataFrame()\n",
    "    for label in range(len(class_dict)):\n",
    "        for threshold in thresholds:\n",
    "            true_label = np.where(preds[\"true_label\"] == label, 1, 0)\n",
    "            pred_label = np.where(preds[f\"raw_pred_{label}\"] > threshold, 1, 0)\n",
    "            report = classification_report(\n",
    "                true_label, \n",
    "                pred_label, \n",
    "                output_dict=True,\n",
    "                zero_division = 0\n",
    "            )\n",
    "\n",
    "            row = pd.DataFrame({\n",
    "                \"label\": [label],\n",
    "                \"threshold\": [threshold],\n",
    "                \"precision\": [report[\"macro avg\"][\"precision\"]],\n",
    "                \"recall\": [report[\"macro avg\"][\"recall\"]],\n",
    "                \"f1-score\": [report[\"macro avg\"][\"f1-score\"]]\n",
    "            })\n",
    "            \n",
    "            performances = pd.concat([performances, row])\n",
    "\n",
    "    performances = performances.reset_index(drop=True)\n",
    "\n",
    "    return performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d769a91-1add-4f5e-9068-d1755ae1bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_thresholds(performances, class_dict):\n",
    "    \"\"\"\n",
    "    Returns the optimum threshold for every class (in argmentation strategy)\n",
    "    including the F1 score, precision and recall of the classifier in a\n",
    "    one vs. all prediction scenario.\n",
    "    \"\"\"\n",
    "    optima = pd.DataFrame()\n",
    "    for label in class_dict.keys():\n",
    "        subset = performances[performances[\"label\"] == label]\n",
    "        idx = subset.idxmax()[\"f1-score\"]\n",
    "        optimum = subset.loc[idx]\n",
    "        optima = pd.concat([optima, pd.DataFrame(optimum).transpose()])\n",
    "\n",
    "    optima[\"label\"] = optima[\"label\"].astype(int)\n",
    "    optima[\"label\"] = optima[\"label\"].replace(class_dict)\n",
    "    optima = optima.rename(columns={\"threshold\": \"optimum_threshold\"})\n",
    "\n",
    "    return optima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82447300-8467-44fe-a3ee-97855b7aa668",
   "metadata": {},
   "source": [
    "## Apply Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195c0ac-2761-428a-afdb-c19041348943",
   "metadata": {},
   "source": [
    "Search for the optimum threshold to binarize predictions for all classes in argumentation strategy in a one vs. all other classes scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17bd5221-0cec-4fe8-94f6-d23ad6fa5abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"twitter-xlm-roberta-base_epochs-100_batchsize-64_strategy\"\n",
    "class_dict = {\n",
    "    0:\"construct\",\n",
    "    1:\"opin\",\n",
    "    2:\"sarc\",\n",
    "    3:\"leave_fact\",\n",
    "    4:\"other\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce861b5c-baf1-4866-85d7-6d10659b6a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model for argumentation startegy overall is split 3.\n"
     ]
    }
   ],
   "source": [
    "# First, we retrieve the best out of five models for argumentation strategy overall.\n",
    "best_split = get_best_split(src, model, class_dict)\n",
    "print(f\"Best model for argumentation startegy overall is split {best_split}.\")\n",
    "# Second, we scan through the thresholds in increments of 0.01 for each of opinion - not opinion, construct - not construct, etc.\n",
    "performances = scan_thresholds(src, model, class_dict, best_split)\n",
    "# Third, we retrieve the thresholds where the F1 score is the highest for each of the classes.\n",
    "optima = get_optimal_thresholds(performances, class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb975fb-54c8-434a-b485-065b7aa309e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"label\", \"optimum_threshold\", \"f1-score\", \"precision\", \"recall\"]\n",
    "optima = optima[cols].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a098c5f-2fda-472a-b455-2a853562b01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>optimum_threshold</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>construct</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.739928</td>\n",
       "      <td>0.713683</td>\n",
       "      <td>0.780287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opin</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.782448</td>\n",
       "      <td>0.789912</td>\n",
       "      <td>0.776485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sarc</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.670686</td>\n",
       "      <td>0.663035</td>\n",
       "      <td>0.679363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave_fact</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.839352</td>\n",
       "      <td>0.840664</td>\n",
       "      <td>0.838074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.884361</td>\n",
       "      <td>0.907378</td>\n",
       "      <td>0.866389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  optimum_threshold  f1-score  precision    recall\n",
       "0   construct               0.27  0.739928   0.713683  0.780287\n",
       "1        opin               0.40  0.782448   0.789912  0.776485\n",
       "2        sarc               0.28  0.670686   0.663035  0.679363\n",
       "3  leave_fact               0.47  0.839352   0.840664  0.838074\n",
       "4       other               0.31  0.884361   0.907378  0.866389"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display optimal thresholds\n",
    "optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066847a-9f97-4d36-8769-9396ce5f4d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
